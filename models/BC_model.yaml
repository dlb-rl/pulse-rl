# behavior cloning model
algorithm: BC
env: ac-pulse
dataset_version: ray
trainer_version: RAY

train:
    dataset_name: 10k_top_500k
    training_iteration: 50000
    time_total: 180000000000
    checkpoint_frequency: 500
    checkpoint_at_end: True
    analysis_metric: episode_reward_mean
    analysis_mode: max
    load_experiment_checkpoint: False
    train_id: Null

evaluation:
    datasets_name: ["10k_uniform_dataset"]
    num_workers: 6
    evaluation_episodes: 0
    rewards_shift: []
    is_gamma: 0.99
    wis_gamma: 0.99
    use_direct_estimator: True
    use_custom_estimator: True

# These are the modified BC_DEFAULT_CONFIG found in ray.rllib.agents.marwil.bc
config:
    train_batch_size: 512
    rollout_fragment_length: 512
    replay_buffer_size: 1000
    learning_starts: 0
    env_config:
        obs_shape: 63
    model:
        fcnet_hiddens: [256, 128, 64]
        fcnet_activation: swish
        conv_filters: Null
        conv_activation: relu
        free_log_std: False
        no_final_linear: False
        vf_share_layers: False
        use_lstm: False
        max_seq_len: 20
        lstm_cell_size: 256
        lstm_use_prev_action_reward: False
        _time_major: False
        framestack: True
        dim: 84
        grayscale: False
        zero_mean: True
        custom_model: Null
        custom_model_config: {}
        custom_action_dist: Null
        custom_preprocessor: Null
    lr: 0.0005
    lr_schedule: [[0, 0.0005], [5000000000, 0.000000000001]]
    gamma: 0.99
    beta: 0.0
    vf_coeff: 1.0
    replay_sequence_length: 1
    input_evaluation: ["is", "wis"]
    evaluation_interval: Null
    evaluation_num_episodes: 10
    evaluation_config:
        input: []
    evaluation_num_workers: 0
    in_evaluation: False
    custom_eval_function: Null
    input: []
    env: Null
    num_workers: 7
    num_envs_per_worker: 1
    num_cpus_per_worker: 1
    num_cpus_for_driver: 1
    num_gpus: 0
    num_gpus_per_worker: 0
    create_env_on_driver: False
    batch_mode: complete_episodes
    optimizer: adam
    horizon: Null
    soft_horizon: False
    no_done_at_end: False
    normalize_actions: False
    clip_rewards: Null
    clip_actions: True
    preprocessor_pref: deepmind
    monitor: False
    log_level: WARN
    # callbacks: ray.rllib.agents.callbacks.DefaultCallbacks
    ignore_worker_failures: False
    log_sys_usage: True
    fake_sampler: False
    framework: tf
    eager_tracing: False
    explore: True
    exploration_config:
        type: StochasticSampling
    sample_async: False
    _use_trajectory_view_api: False
    observation_filter: NoFilter
    synchronize_filters: True
    tf_session_args:
        intra_op_parallelism_threads: 2
        inter_op_parallelism_threads: 2
        gpu_options:
            allow_growth: True
        log_device_placement: False
        device_count:
            CPU: 1
        allow_soft_placement: True
    local_tf_session_args:
        intra_op_parallelism_threads: 8
        inter_op_parallelism_threads: 8
    compress_observations: False
    collect_metrics_timeout: 180
    metrics_smoothing_episodes: 100
    remote_worker_envs: False
    remote_env_batch_wait_ms: 0
    min_iter_time_s: 0
    timesteps_per_iteration: 0
    seed: Null
    extra_python_environs_for_driver: {}
    extra_python_environs_for_worker: {}
    custom_resources_per_worker: {}
    object_store_memory: 0
    memory: 0
    memory_per_worker: 0
    object_store_memory_per_worker: 0
    postprocess_inputs: True
    shuffle_buffer_size: 0
    output: Null
    output_compress_columns: ["obs", "new_obs"]
    output_max_file_size: 67108864
    multiagent:
        policies: {}
        policy_mapping_fn: Null
        policies_to_train: Null
        observation_fn: Null
        replay_mode: independent
    logger_config: Null
